{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# plt.style.use('seaborn-paper')\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "# plt.rcParams['font.family']='Times New Roman,Microsoft YaHei'# 设置字体族，中文为微软雅黑，英文为Times New Roman\n",
    "plt.rcParams['font.sans-serif'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'  # 设置数学公式字体为stix\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datajh=pd.read_csv(\"groupedjhS.csv\")\n",
    "datacm=pd.read_csv(\"groupedcmS.csv\")\n",
    "datajhsoa=pd.read_csv(\"groupedjhSOA.csv\")\n",
    "datacmsoa=pd.read_csv(\"groupedcmSOA.csv\")"
   ],
   "id": "cc6e99f7ffcedf13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# datajh=pd.read_csv(\"groupedjhall.csv\")\n",
    "# datacm=pd.read_csv(\"groupedcmall.csv\")\n",
    "# datajhsoa=datajh\n",
    "# datacmsoa=datacm\n",
    "# \n",
    "# datajhsoa[\"SOA\"] = 0\n",
    "# datajh[\"SOA\"] = 0\n",
    "# for i in datajhsoa.columns[(datajhsoa.columns.get_loc(\"0.25um\")):(datajhsoa.columns.get_loc(\"0.28um\"))]:\n",
    "#     datajhsoa[\"SOA\"] = datajhsoa[\"SOA\"] + datajhsoa[i]\n",
    "#     datajh[\"SOA\"] = datajh[\"SOA\"] + datajh[i]\n",
    "# datacmsoa[\"SOA\"] = 0\n",
    "# datacm[\"SOA\"] = 0\n",
    "# for i in datacmsoa.columns[(datacmsoa.columns.get_loc(\"0.25um\")):(datacmsoa.columns.get_loc(\"0.28um\"))]:\n",
    "#     datacmsoa[\"SOA\"] = datacmsoa[\"SOA\"] + datacmsoa[i]\n",
    "#     datacm[\"SOA\"] = datacm[\"SOA\"] + datacm[i]"
   ],
   "id": "814f07faec2acf87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VOCs = ['Methyl Mercaptan', '1,3-Butadiene', 'Butene', 'Acetone/Butane', 'n-Propanol', 'Dimethyl Sulfide/Ethyl Mercaptan', 'Chloroethane', 'Isoprene', 'Pentene', 'Pentane/Isopentane', 'Dimethylformamide', 'Ethyl Formate', 'Carbon Disulfide/Propyl Mercaptan', 'Benzene', 'Cyclohexene', 'Hexene/Methylcyclopentane', 'n-Hexane/Dimethylbutane', 'Ethyl Sulfide/Butyl Mercaptan', 'Toluene', 'Aniline', 'Dimethyl Disulfide', '1,1-Dichloroethylene', 'Methylcyclohexane', 'n-Heptane', 'Triethylamine', 'n-Propyl Acetate', 'Diethylene Triamine', 'Styrene', 'Xylene/Ethylbenzene', '1,3-Dichloropropene', 'n-Octane', 'n-Butyl Acetate', 'Hexyl Mercaptan', 'Xylenol', 'Trichloroethylene', 'Diethylbenzene', 'Methyl Benzoate', 'Trimethyl Phosphate', 'n-Decanol', 'Dichlorobenzene', 'Diethyl Aniline', 'Undecane', 'Tetrachloroethylene', 'n-Dodecane', 'Dibromomethane', '1,2,4-Trichlorobenzene', 'n-Tridecane', '1,2-Dibromoethane']",
   "id": "dd0a2346dcb282cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "datajhsoa[\"SOA\"] = 0\n",
    "datajh[\"SOA\"] = 0\n",
    "for i in datajhsoa.columns[(datajhsoa.columns.get_loc(\"0.25um\")):(datajhsoa.columns.get_loc(\"0.28um\"))]:\n",
    "    datajhsoa[\"SOA\"] = datajhsoa[\"SOA\"] + datajhsoa[i]\n",
    "    datajh[\"SOA\"] = datajh[\"SOA\"] + datajh[i]\n",
    "datajhsoa[\"CSOA\"] = 0\n",
    "for i in datajhsoa.columns[(datajhsoa.columns.get_loc(\"C0.25um\")):(datajhsoa.columns.get_loc(\"C0.28um\"))]:\n",
    "    datajhsoa[\"CSOA\"] = datajhsoa[\"CSOA\"] + datajhsoa[i]\n",
    "datacmsoa[\"SOA\"] = 0\n",
    "datacm[\"SOA\"] = 0\n",
    "for i in datacmsoa.columns[(datacmsoa.columns.get_loc(\"0.25um\")):(datacmsoa.columns.get_loc(\"0.28um\"))]:\n",
    "    datacmsoa[\"SOA\"] = datacmsoa[\"SOA\"] + datacmsoa[i]\n",
    "    datacm[\"SOA\"] = datacm[\"SOA\"] + datacm[i]\n",
    "datacmsoa[\"CSOA\"] = 0\n",
    "for i in datacmsoa.columns[(datacmsoa.columns.get_loc(\"C0.25um\")):(datacmsoa.columns.get_loc(\"C0.28um\"))]:\n",
    "    datacmsoa[\"CSOA\"] = datacmsoa[\"CSOA\"] + datacmsoa[i]"
   ],
   "id": "e5c7fd72dceb6bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datajh['place']='JH'\n",
    "datacm['place']='CM'\n",
    "dataall=pd.concat([datajh,datacm],axis=0)\n",
    "dataall.columns = ['Time', 'TVOCs', 'Methyl Mercaptan', '1,3-Butadiene', 'Butene', 'Acetone/Butane', 'n-Propanol',\n",
    "                   'Dimethyl Sulfide/Ethyl Mercaptan', 'Chloroethane', 'Isoprene', 'Pentene', 'Pentane/Isopentane',\n",
    "                   'Dimethylformamide', 'Ethyl Formate', 'Carbon Disulfide/Propyl Mercaptan', 'Benzene', 'Cyclohexene',\n",
    "                   'Hexene/Methylcyclopentane', 'n-Hexane/Dimethylbutane', 'Ethyl Sulfide/Butyl Mercaptan', 'Toluene',\n",
    "                   'Aniline', 'Dimethyl Disulfide', '1,1-Dichloroethylene', 'Methylcyclohexane', 'n-Heptane',\n",
    "                   'Triethylamine', 'n-Propyl Acetate', 'Diethylene Triamine', 'Styrene', 'Xylene/Ethylbenzene',\n",
    "                   '1,3-Dichloropropene', 'n-Octane', 'n-Butyl Acetate', 'Hexyl Mercaptan', 'Xylenol',\n",
    "                   'Trichloroethylene', 'Diethylbenzene', 'Methyl Benzoate', 'Trimethyl Phosphate', 'n-Decanol',\n",
    "                   'Dichlorobenzene', 'Diethyl Aniline', 'Undecane', 'Tetrachloroethylene', 'n-Dodecane',\n",
    "                   'Dibromomethane', '1,2,4-Trichlorobenzene', 'n-Tridecane', '1,2-Dibromoethane', '0.25um', '0.28um',\n",
    "                   '0.30um', '0.35um', '0.40um', '0.45um', '0.50um', '0.58um', '0.65um', '0.70um', '0.80um', '1.00um',\n",
    "                   '1.30um', '1.60um', '2.00um', '2.50um', '3.00um', '3.50um', '4.00um', '5.00um', '6.50um', '7.50um',\n",
    "                   '8.50um', '10.00um', '12.50um', '15.00um', '17.50um', '20.00um', '25.00um', '30.00um', '32.00um',\n",
    "                   'PM10', 'PM2.5', 'PM1', 'SO2', 'NOx', 'NO', 'NO2', 'CO', 'O3', 'NO2.1', 'NegativeOxygenIons',\n",
    "                   'Radiation', 'Temperature', 'Humidity', 'WindSpeed', 'Hour_Min_Sec', 'Hour_Min', 'Hour', 'Month',\n",
    "                   'Day', 'Datetime', 'seconds', 'SOA', 'place']\n",
    "dataall"
   ],
   "id": "4772acd6424f7a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import norm, expon, gamma, lognorm, beta, kstest, shapiro\n",
    "def normal_distribution_fit_and_test(grouped_by_hour):\n",
    "    normality_results = {}\n",
    "    for hour, group in grouped_by_hour:\n",
    "        # Fit normal distribution and get parameters\n",
    "        params = norm.fit(group)\n",
    "        # Perform Shapiro-Wilk test\n",
    "        _, p_value = shapiro(group)\n",
    "        # Check if data is normally distributed based on p-value\n",
    "        is_normal = True if p_value > 0.05 else False\n",
    "        normality_results[hour] = (\n",
    "        params[0], params[1], p_value, is_normal)  # params[0]: mean, params[1]: standard deviation\n",
    "\n",
    "    normality_df = pd.DataFrame(normality_results).T\n",
    "    normality_df.columns = [\"Mean\", \"Standard Deviation\", \"P-Value\", \"Is Normal\"]\n",
    "    normality_df.reset_index(inplace=True)\n",
    "    normality_df.rename(columns={'index': 'Hour'}, inplace=True)\n",
    "\n",
    "    return normality_df\n",
    "\n"
   ],
   "id": "9eec837b3b4d3a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# 定义与均值 Mean 的关系的函数\n",
    "def mean_relation(T, Q0, a, v0):\n",
    "    return Q0 + (a * T ** 2) / 2 + T * v0\n",
    "\n",
    "\n",
    "# 定义与方差 variance 的关系的函数\n",
    "def std_dev_relation(T, k, sigma0):\n",
    "    return ((k ** 2) * (T ** 3)) / 3 + k * (T ** 2) * sigma0 + T * (sigma0 ** 2)\n",
    "\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "# 定义去除离群值的函数（使用 Z-score 方法）\n",
    "def remove_outliers(data):\n",
    "    z_scores = zscore(data)\n",
    "    return data[(np.abs(z_scores) < 3)]  # 通常使用 3 作为 Z-score 的阈值\n",
    "\n",
    "\n",
    "# 定义加权二次关系模型\n",
    "def weighted_quadratic_relation(T, y, weights):\n",
    "    # 增加常数项和二次项\n",
    "    T_with_const = sm.add_constant(np.column_stack((T, T**2)))\n",
    "    # 加权最小二乘法（WLS）\n",
    "    model_wls = sm.WLS(y, T_with_const, weights=weights).fit()\n",
    "    return model_wls.rsquared  # 返回加权的 R² 值\n",
    "\n",
    "def clean_data(df, columns, threshold=3):\n",
    "    for col in columns:\n",
    "        df = df[np.abs(zscore(df[col])) < threshold]\n",
    "    return df.reset_index(drop=True)\n"
   ],
   "id": "2b0ef7768b197c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from scipy.spatial import ConvexHull\n",
    "# from shapely.geometry import Polygon\n",
    "# from shapely.ops import unary_union\n",
    "# from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "# def compute_area(points):\n",
    "#     # 计算点的凸包区域\n",
    "#     hull = ConvexHull(points)\n",
    "#     polygon = Polygon(points[hull.vertices])\n",
    "#     return polygon.area\n",
    "# \n",
    "# \n",
    "# # 蒙特卡罗模拟方法计算重叠面积和综合拟合度\n",
    "# def monte_carlo_r_squared_area(T, mean_params, std_dev_params, real_data, num_simulations=1000, w=0, penalty_coefficient=1.0):\n",
    "#     original_points = np.column_stack((T, real_data))\n",
    "#     original_area = compute_area(original_points)\n",
    "# \n",
    "#     simulated_means = mean_relation(T, *mean_params)\n",
    "#     residuals = real_data - simulated_means\n",
    "#     ss_res = np.sum(residuals ** 2)\n",
    "#     ss_tot = np.sum((real_data - np.mean(real_data)) ** 2)\n",
    "#     r_squared_mean = 1 - (ss_res / ss_tot)\n",
    "# \n",
    "#     real_var = np.var(real_data)\n",
    "#     simulation_areas = []\n",
    "#     overlap_areas = []\n",
    "#     penalty_total = 0\n",
    "# \n",
    "#     for _ in range(num_simulations):\n",
    "#         simulated_path = norm.rvs(\n",
    "#             loc=mean_relation(T, *mean_params),\n",
    "#             scale=np.sqrt(std_dev_relation(T, *std_dev_params)),\n",
    "#             size=len(T)\n",
    "#         )\n",
    "#         simulated_points = np.column_stack((T, simulated_path))\n",
    "#         sim_area = compute_area(simulated_points)\n",
    "#         simulation_areas.append(sim_area)\n",
    "# \n",
    "#         # 计算模拟路径的方差，并计算惩罚项\n",
    "#         sim_var = np.var(simulated_path)\n",
    "#         penalty = max(0, (sim_var - real_var) / real_var)  # 方差相对偏差\n",
    "#         penalty_total += penalty\n",
    "# \n",
    "#         original_polygon = Polygon(original_points[ConvexHull(original_points).vertices])\n",
    "#         simulated_polygon = Polygon(simulated_points[ConvexHull(simulated_points).vertices])\n",
    "#         intersection_area = original_polygon.intersection(simulated_polygon).area\n",
    "#         overlap_areas.append(intersection_area)\n",
    "# \n",
    "#     mean_overlap_area = np.mean(overlap_areas)\n",
    "# \n",
    "#     # 调整后的 R² 计算\n",
    "#     r_squared_adjusted_area = mean_overlap_area / (original_area + penalty_coefficient * penalty_total)\n",
    "#     r_squared_adjusted_area = min(max(r_squared_adjusted_area, 0), 1)  # 限制 R² 在 [0, 1]\n",
    "# \n",
    "#     # 综合 R²\n",
    "#     r_squared_combined = w * r_squared_mean + (1 - w) * r_squared_adjusted_area\n",
    "#     return r_squared_combined\n",
    "\n",
    "# def monte_carlo_r_squared_area(T, mean_params, std_dev_params, real_data, num_simulations=1000):\n",
    "#     original_points = np.column_stack((T, real_data))\n",
    "#     original_area = compute_area(original_points)\n",
    "# \n",
    "#     simulation_areas = []\n",
    "#     overlap_areas = []\n",
    "# \n",
    "#     for _ in range(num_simulations):\n",
    "#         simulated_path = norm.rvs(\n",
    "#             loc=mean_relation(T, *mean_params),\n",
    "#             scale=np.sqrt(std_dev_relation(T, *std_dev_params)),\n",
    "#             size=len(T)\n",
    "#         )\n",
    "#         simulated_points = np.column_stack((T, simulated_path))\n",
    "#         sim_area = compute_area(simulated_points)\n",
    "#         simulation_areas.append(sim_area)\n",
    "# \n",
    "#         original_polygon = Polygon(original_points[ConvexHull(original_points).vertices])\n",
    "#         simulated_polygon = Polygon(simulated_points[ConvexHull(simulated_points).vertices])\n",
    "#         intersection_area = original_polygon.intersection(simulated_polygon).area\n",
    "#         overlap_areas.append(intersection_area)\n",
    "# \n",
    "#     mean_overlap_area = np.mean(overlap_areas)\n",
    "#     mean_simulation_area = np.mean(simulation_areas)\n",
    "# \n",
    "#     # Calculate R² based on area overlap\n",
    "#     r_squared_area = mean_overlap_area / original_area\n",
    "#     return min(max(r_squared_area, 0), 1)  # Ensure R² is in [0, 1]\n"
   ],
   "id": "77b65ea07f291571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from scipy.spatial import ConvexHull\n",
    "# from shapely.geometry import Polygon\n",
    "# from shapely.ops import unary_union\n",
    "# from scipy.stats import norm\n",
    "# \n",
    "# def compute_area(points):\n",
    "#     \"\"\"\n",
    "#     计算点集的凸包区域面积。\n",
    "#     \"\"\"\n",
    "#     if len(points) < 3:\n",
    "#         return 0\n",
    "#     hull = ConvexHull(points)\n",
    "#     polygon = Polygon(points[hull.vertices])\n",
    "#     return polygon.area\n",
    "# \n",
    "# def monte_carlo_r_squared_area(\n",
    "#     T,\n",
    "#     mean_params,\n",
    "#     std_dev_params,\n",
    "#     real_data,\n",
    "#     num_simulations=1000,\n",
    "#     penalty_coefficient=1.0\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     蒙特卡罗模拟方法计算调整后的 R²，考虑新的方差惩罚项。\n",
    "#     T: 自变量（例如温度）\n",
    "#     mean_params: 拟合的均值函数参数\n",
    "#     std_dev_params: 拟合的方差函数参数\n",
    "#     real_data: 原始观测数据\n",
    "#     num_simulations: 模拟次数\n",
    "#     penalty_coefficient: 惩罚系数，用于调整方差惩罚的影响程度\n",
    "#     \"\"\"\n",
    "#     # 原始数据点\n",
    "#     original_points = np.column_stack((T, real_data))\n",
    "#     original_area = compute_area(original_points)\n",
    "# \n",
    "#     # 原始数据的方差\n",
    "#     real_var = np.var(real_data)\n",
    "# \n",
    "#     overlap_areas = []\n",
    "#     penalties = []\n",
    "# \n",
    "#     for _ in range(num_simulations):\n",
    "#         # 根据均值和方差函数生成模拟数据\n",
    "#         simulated_path = norm.rvs(\n",
    "#             loc=mean_relation(T, *mean_params),\n",
    "#             scale=np.sqrt(std_dev_relation(T, *std_dev_params)),\n",
    "#             size=len(T)\n",
    "#         )\n",
    "#         simulated_points = np.column_stack((T, simulated_path))\n",
    "# \n",
    "#         # 计算模拟数据与原始数据的重叠面积\n",
    "#         original_polygon = Polygon(original_points[ConvexHull(original_points).vertices])\n",
    "#         simulated_polygon = Polygon(simulated_points[ConvexHull(simulated_points).vertices])\n",
    "#         intersection = original_polygon.intersection(simulated_polygon)\n",
    "#         intersection_area = intersection.area if not intersection.is_empty else 0\n",
    "#         overlap_areas.append(intersection_area)\n",
    "# \n",
    "#         # 计算模拟数据的方差，计算惩罚项\n",
    "#         sim_var = np.var(simulated_path)\n",
    "#         if sim_var > real_var:\n",
    "#             penalty = ((sim_var / real_var) - 1) ** 2  # 方差比的平方惩罚\n",
    "#         else:\n",
    "#             penalty = 0  # 如果模拟方差不超过真实方差，则无惩罚\n",
    "#         penalties.append(penalty)\n",
    "# \n",
    "#     # 计算平均重叠面积和平均惩罚\n",
    "#     mean_overlap_area = np.mean(overlap_areas)\n",
    "#     mean_penalty = np.mean(penalties)\n",
    "# \n",
    "#     # 计算调整后的 R²，考虑新的方差惩罚项\n",
    "#     adjusted_r_squared = mean_overlap_area / (original_area * (1 + penalty_coefficient * mean_penalty))\n",
    "#     adjusted_r_squared = np.clip(adjusted_r_squared, 0, 1)  # 限制 R² 在 [0, 1] 之间\n",
    "# \n",
    "#     return adjusted_r_squared"
   ],
   "id": "ea38350e6c02da53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from scipy.stats import gaussian_kde, norm\n",
    "# \n",
    "# \n",
    "# def kl_divergence(p, q, x_vals):\n",
    "#     \"\"\"\n",
    "#     计算两个概率密度之间的KL散度\n",
    "#     \"\"\"\n",
    "#     return np.sum(p * np.log(p / (q + 1e-10))) * (x_vals[1] - x_vals[0])\n",
    "# \n",
    "# def monte_carlo_density_r_squared(\n",
    "#     T,\n",
    "#     mean_params,\n",
    "#     std_dev_params,\n",
    "#     real_data,\n",
    "#     num_simulations=1000,\n",
    "#     penalty_coefficient=1\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     基于概率密度的蒙特卡罗模拟方法计算调整后的 R²，考虑方差惩罚。\n",
    "#     T: 自变量（例如温度）\n",
    "#     mean_params: 拟合的均值函数参数\n",
    "#     std_dev_params: 拟合的方差函数参数\n",
    "#     real_data: 原始观测数据\n",
    "#     num_simulations: 模拟次数\n",
    "#     penalty_coefficient: 惩罚系数，用于调整方差惩罚的影响程度\n",
    "#     \"\"\"\n",
    "#     # 计算观测数据的概率密度\n",
    "#     density_obs = gaussian_kde(real_data)\n",
    "#     x_vals = np.linspace(min(real_data), max(real_data), 100)\n",
    "#     p_obs = density_obs(x_vals)\n",
    "#     \n",
    "#     # 计算空模型（正态分布）的概率密度\n",
    "#     mean_obs, std_obs = np.mean(real_data), np.std(real_data)\n",
    "#     p_null = norm.pdf(x_vals, loc=mean_obs, scale=std_obs)\n",
    "#     kl_null = kl_divergence(p_obs, p_null, x_vals)\n",
    "# \n",
    "#     # 模拟路径的KL散度和方差惩罚\n",
    "#     kl_total = 0\n",
    "#     penalties = []\n",
    "# \n",
    "#     for _ in range(num_simulations):\n",
    "#         # 生成模拟路径\n",
    "#         simulated_path = norm.rvs(\n",
    "#             loc=mean_relation(T, *mean_params),\n",
    "#             scale=np.sqrt(std_dev_relation(T, *std_dev_params)),\n",
    "#             size=len(T)\n",
    "#         )\n",
    "#         \n",
    "#         # 计算模拟路径的概率密度\n",
    "#         density_sim = gaussian_kde(simulated_path)\n",
    "#         p_sim = density_sim(x_vals)\n",
    "#         \n",
    "#         # 计算观测数据与模拟数据的KL散度\n",
    "#         kl_sim = kl_divergence(p_obs, p_sim, x_vals)\n",
    "#         kl_total += kl_sim\n",
    "#         \n",
    "#         # 计算模拟路径的方差，并计算方差惩罚\n",
    "#         sim_var = np.var(simulated_path)\n",
    "#         real_var = np.var(real_data)\n",
    "#         if sim_var > real_var:\n",
    "#             penalty = ((sim_var / real_var) - 1) ** 2  # 方差比的平方惩罚\n",
    "#         else:\n",
    "#             penalty = 0  # 如果模拟方差不超过真实方差，则无惩罚\n",
    "#         penalties.append(penalty)\n",
    "# \n",
    "#     # 计算平均KL散度和平均方差惩罚\n",
    "#     kl_avg = kl_total / num_simulations\n",
    "#     mean_penalty = np.mean(penalties)\n",
    "# \n",
    "#     # 计算调整后的 R²，考虑KL散度和方差惩罚\n",
    "#     adjusted_r_squared = 1 - (kl_avg / kl_null) / (1 + penalty_coefficient * mean_penalty)\n",
    "#     adjusted_r_squared = np.clip(adjusted_r_squared, 0, 1)  # 限制 R² 在 [0, 1] 之间\n",
    "# \n",
    "#     return adjusted_r_squared\n"
   ],
   "id": "dcbf7df262a9fb5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:57:00.209042Z",
     "start_time": "2024-11-10T06:57:00.181377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import (\n",
    "    gaussian_kde,\n",
    "    norm,\n",
    "    uniform,\n",
    "    gamma,\n",
    "    weibull_min,\n",
    "    pearson3  # 用于对数皮尔逊III型分布\n",
    ")\n",
    "\n",
    "def kl_divergence(p, q, x_vals):\n",
    "    \"\"\"\n",
    "    计算两个概率密度之间的KL散度\n",
    "    \"\"\"\n",
    "    return np.sum(p * np.log(p / (q + 1e-10))) * (x_vals[1] - x_vals[0])\n",
    "\n",
    "def adaptive_penalty_coefficient(sim_var, real_var):\n",
    "    variance_ratio = sim_var / real_var\n",
    "    if variance_ratio > 1:\n",
    "        return np.exp(variance_ratio)\n",
    "    else:\n",
    "        return np.log(variance_ratio+1) / np.log(100)  # 如果模拟方差不超过真实方差，则轻微惩罚\n",
    "        # return 0  \n",
    "\n",
    "def monte_carlo_density_r_squared(\n",
    "    T,\n",
    "    mean_params,\n",
    "    std_dev_params,\n",
    "    real_data,\n",
    "    num_simulations=1000,\n",
    "    distribution='uniform'  # 新增参数，默认为 'uniform'\n",
    "):\n",
    "    \"\"\"\n",
    "    基于概率密度的蒙特卡罗模拟方法计算调整后的 R²，考虑自适应的方差惩罚。\n",
    "\n",
    "    T: 自变量（例如温度）\n",
    "    mean_params: 拟合的均值函数参数\n",
    "    std_dev_params: 拟合的方差函数参数\n",
    "    real_data: 原始观测数据\n",
    "    num_simulations: 模拟次数\n",
    "    distribution: 零模型的分布类型，支持 'normal', 'uniform', 'gamma', 'weibull', 'logpearson3'\n",
    "    \"\"\"\n",
    "    # 计算观测数据的概率密度\n",
    "    density_obs = gaussian_kde(real_data)\n",
    "    x_vals = np.linspace(np.min(real_data), np.max(real_data), 100)\n",
    "    p_obs = density_obs(x_vals)\n",
    "    \n",
    "    simv=[]\n",
    "    ov=[]\n",
    "\n",
    "    # 根据指定的分布类型计算零模型的概率密度\n",
    "    if distribution == 'normal':\n",
    "        mean_obs, std_obs = np.mean(real_data), np.std(real_data)\n",
    "        p_null = norm.pdf(x_vals, loc=mean_obs, scale=std_obs)\n",
    "    elif distribution == 'uniform':\n",
    "        a, b = np.min(real_data), np.max(real_data)\n",
    "        p_null = uniform.pdf(x_vals, loc=a, scale=b - a)\n",
    "    elif distribution == 'gamma':\n",
    "        params = gamma.fit(real_data, floc=0)  # 固定 loc=0，避免负值\n",
    "        shape, loc, scale = params\n",
    "        p_null = gamma.pdf(x_vals, a=shape, loc=loc, scale=scale)\n",
    "    elif distribution == 'weibull':\n",
    "        params = weibull_min.fit(real_data, floc=0)  # 固定 loc=0\n",
    "        c, loc, scale = params\n",
    "        p_null = weibull_min.pdf(x_vals, c=c, loc=loc, scale=scale)\n",
    "    elif distribution == 'logpearson3':\n",
    "        # 对数据取对数\n",
    "        positive_data = real_data[real_data > 0]  # 确保数据为正\n",
    "        log_data = np.log(positive_data)\n",
    "        # 拟合 Pearson III 型分布\n",
    "        skew, loc, scale = pearson3.fit(log_data)\n",
    "        # 计算对数空间的概率密度\n",
    "        log_x_vals = np.log(x_vals[x_vals > 0])  # x_vals 也要为正\n",
    "        p_null_log = pearson3.pdf(log_x_vals, skew=skew, loc=loc, scale=scale)\n",
    "        # 转换回原始空间的概率密度\n",
    "        p_null = np.zeros_like(x_vals)\n",
    "        p_null[x_vals > 0] = (1 / x_vals[x_vals > 0]) * p_null_log\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的分布类型：{distribution}\")\n",
    "\n",
    "    kl_null = kl_divergence(p_obs, p_null, x_vals)\n",
    "\n",
    "    # 模拟路径的KL散度和自适应惩罚\n",
    "    kl_total = 0\n",
    "    penalty_total = 0\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # 生成模拟路径\n",
    "        simulated_path = np.random.normal(\n",
    "            loc=mean_relation(T, *mean_params),\n",
    "            scale=np.sqrt(std_dev_relation(T, *std_dev_params)),\n",
    "            size=len(T)\n",
    "        )\n",
    "        \n",
    "        # 计算模拟路径的概率密度\n",
    "        density_sim = gaussian_kde(simulated_path)\n",
    "        p_sim = density_sim(x_vals)\n",
    "        \n",
    "        # 计算观测数据与模拟数据的KL散度\n",
    "        kl_sim = kl_divergence(p_obs, p_sim, x_vals)\n",
    "        kl_total += kl_sim\n",
    "        \n",
    "        # 计算模拟路径的方差，并计算自适应惩罚\n",
    "        sim_var = np.var(simulated_path)\n",
    "        real_var = np.var(real_data)\n",
    "        penalty = adaptive_penalty_coefficient(sim_var, real_var)\n",
    "        penalty_total += penalty\n",
    "        simv.append(sim_var)\n",
    "        ov.append(real_var)\n",
    "\n",
    "    # 计算平均KL散度和平均自适应惩罚\n",
    "    kl_avg = kl_total / num_simulations\n",
    "    mean_penalty = penalty_total / num_simulations\n",
    "\n",
    "    \n",
    "    print(np.mean(simv)/np.mean(ov))\n",
    "\n",
    "    # 计算调整后的 R²，考虑自适应的方差惩罚\n",
    "    adjusted_r_squared = (1 - (kl_avg / kl_null)) / (1 + mean_penalty)\n",
    "    print(adjusted_r_squared)\n",
    "    adjusted_r_squared = np.clip(adjusted_r_squared, 0, 1)  # 限制 R² 在 [0, 1] 之间\n",
    "\n",
    "    return adjusted_r_squared"
   ],
   "id": "6f68fd3886de5593",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载已保存的 KMeans 模型\n",
    "kmeans = joblib.load('kmeans.joblib')\n",
    "# 获取每个化学物质的聚类标签并存储为字典\n",
    "chemical_clusters = dict(zip(VOCs, kmeans.labels_))\n",
    "\n",
    "\n",
    "# 定义处理化学物质数据的函数\n",
    "def process_chemical(data, chemical_name):\n",
    "    print(chemical_name)\n",
    "    data_grouped = data.groupby([\"Hour_Min\"]).mean(numeric_only=True).reset_index()\n",
    "    data_grouped[\"Concentration\"] = data_grouped[chemical_name]\n",
    "    grouped_by_hour = data_grouped.groupby('Hour')\n",
    "    normality_df = normal_distribution_fit_and_test(grouped_by_hour[chemical_name])\n",
    "    normality_df['T'] = data_grouped.groupby('Hour').mean(numeric_only=True)[\"Temperature\"]\n",
    "    \n",
    "    # data_grouped = data.copy()\n",
    "    # data_grouped[\"Concentration\"] = data_grouped[chemical_name]\n",
    "    # grouped_by_hour = data_grouped.groupby('Hour_Min')\n",
    "    # normality_df = normal_distribution_fit_and_test(grouped_by_hour[chemical_name])\n",
    "    # normality_df['T'] = data_grouped.groupby('Hour_Min').mean(numeric_only=True)[\"Temperature\"].values\n",
    "\n",
    "    # 提取数据\n",
    "    T = normality_df['T'].values\n",
    "    mean_values = normality_df['Mean'].astype(\"float\").values\n",
    "    std_dev_values = normality_df['Standard Deviation'].astype(\"float\").values ** 2\n",
    "\n",
    "    # 去除离群值\n",
    "    mean_values_filtered = remove_outliers(mean_values)\n",
    "    std_dev_values_filtered = remove_outliers(std_dev_values)\n",
    "    T_filtered_mean = T[np.isin(mean_values, mean_values_filtered)]\n",
    "    T_filtered_std_dev = T[np.isin(std_dev_values, std_dev_values_filtered)]\n",
    "    \n",
    "\n",
    "    # 拟合模型\n",
    "    params_mean, covariance_mean = curve_fit(mean_relation, T_filtered_mean, mean_values_filtered, method='trf', maxfev=1000)\n",
    "    params_std_dev, covariance_std_dev = curve_fit(std_dev_relation, T_filtered_std_dev, std_dev_values_filtered, method='trf', maxfev=1000,\n",
    "    bounds=([0, -np.inf], [np.inf, np.inf]))\n",
    "\n",
    "    # 计算统计量\n",
    "    p_values_mean = calculate_p_values(params_mean, covariance_mean, len(T_filtered_mean))\n",
    "    p_values_std_dev = calculate_p_values(params_std_dev, covariance_std_dev, len(T_filtered_std_dev))\n",
    "    r_squared_mean = calculate_r_squared(mean_values_filtered, mean_relation, T_filtered_mean, params_mean)\n",
    "    r_squared_std_dev = calculate_r_squared(std_dev_values_filtered, std_dev_relation, T_filtered_std_dev, params_std_dev)\n",
    "    \n",
    "    # 计算占比 %\n",
    "    total_mean_concentration = data[VOCs].mean().sum()  # 总体均值\n",
    "    percentage = (np.mean(mean_values_filtered) / total_mean_concentration) * 100  # 占比 %\n",
    "    \n",
    "    cluster_label = chemical_clusters.get(chemical_name, None)\n",
    "    \n",
    "    weight=std_dev_relation(T_filtered_mean, *params_std_dev)\n",
    "    weights = 1 / (weight + 1e-8)\n",
    "    wls_r_squared = weighted_quadratic_relation(T_filtered_mean, mean_values_filtered, weights)\n",
    "    \n",
    "    data_grouped = clean_data(data_grouped, ['Temperature', chemical_name])\n",
    "    print(data_grouped.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 计算伪 R²\n",
    "\n",
    "    rtotal=  monte_carlo_density_r_squared(\n",
    "        data_grouped['Temperature'].values, params_mean, params_std_dev, data_grouped[[chemical_name]].values.T\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    # 整理结果\n",
    "    return {\n",
    "        'Chemical': chemical_name,\n",
    "        'Cluster': cluster_label+1,\n",
    "        'Average Concentration': np.mean(mean_values_filtered),\n",
    "        'R2 Total':rtotal,\n",
    "        'R2 WLS':wls_r_squared,\n",
    "        'R2 Mean': r_squared_mean,\n",
    "        'Concentration Std Dev': np.std(mean_values_filtered),\n",
    "        'R2 Std Dev': r_squared_std_dev,\n",
    "        'Percentage (%)': percentage,\n",
    "        'Q0': params_mean[0],\n",
    "        'Pvalue Q0': p_values_mean[0],\n",
    "        'a': params_mean[1],\n",
    "        'Pvalue a': p_values_mean[1],\n",
    "        'v0': params_mean[2],\n",
    "        'Pvalue v0': p_values_mean[2],\n",
    "        'k': params_std_dev[0],\n",
    "        'Pvalue k': p_values_std_dev[0],\n",
    "        'sigma0': params_std_dev[1],\n",
    "        'Pvalue sigma0': p_values_std_dev[1]\n",
    "    }\n",
    "\n",
    "# 定义用于计算 P 值和 R² 的辅助函数\n",
    "def calculate_p_values(params, covariance, n):\n",
    "    std_errors = np.sqrt(np.diag(covariance))\n",
    "    t_values = params / std_errors\n",
    "    dof = max(0, n - len(params))\n",
    "    p_values = 2 * (1 - t.cdf(np.abs(t_values), dof))\n",
    "    return p_values\n",
    "\n",
    "def calculate_r_squared(data, fit_function, T, params):\n",
    "    residuals = data - fit_function(T, *params)\n",
    "    ss_res = np.sum(residuals ** 2)\n",
    "    ss_tot = np.sum((data - np.mean(data)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n"
   ],
   "id": "d32e1c18d79f483f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb6fb8c6dcadd1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 计算每种VOC的平均浓度\n",
    "average_concentrations = dataall[VOCs].mean()\n",
    "# 计算所有VOCs的平均总浓度\n",
    "total_concentration = average_concentrations.sum()\n",
    "# 计算每种VOC占总浓度的百分比\n",
    "concentration_percentage = (average_concentrations / total_concentration) * 100\n",
    "# 筛选出占比大于2%的VOCs\n",
    "selected_chemicals = concentration_percentage[concentration_percentage > 2].index.tolist()"
   ],
   "id": "e2e037e4eea924b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chemicals = selected_chemicals",
   "id": "9c7d84c54720bccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 处理 JH 地点的化学物质数据\n",
    "results_jh = [process_chemical(dataall[dataall.place == 'JH'], chemical) for chemical in chemicals]\n",
    "results_df_jh = pd.DataFrame(results_jh)\n",
    "results_df_jh['Place'] = 'JH'  # 添加地点信息\n",
    "\n",
    "# 处理 CM 地点的化学物质数据\n",
    "results_cm = [process_chemical(dataall[dataall.place == 'CM'], chemical) for chemical in chemicals]\n",
    "results_df_cm = pd.DataFrame(results_cm)\n",
    "results_df_cm['Place'] = 'CM'  # 添加地点信息\n"
   ],
   "id": "6bc3db5251dee7f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 合并 JH 和 CM 的结果\n",
    "results_df = pd.concat([results_df_jh, results_df_cm], axis=0)\n",
    "\n",
    "results_df.set_index(['Chemical', 'Cluster', 'Place'], inplace=True)\n",
    "\n",
    "jh_r2_mean = results_df.xs('JH', level='Place')['R2 Mean']\n",
    "\n",
    "sorted_index = jh_r2_mean.sort_values(ascending=False).index.get_level_values('Chemical')\n",
    "results_df_sorted = results_df.loc[sorted_index]\n",
    "\n",
    "# 显示结果\n",
    "results_df_sorted"
   ],
   "id": "46459840267067d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c0691e1c9101b6e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d0d01c4a24dbeb2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a11e96c2aaee874c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 保存为 Excel 文件并指定 sheet 名称\n",
    "results_df_sorted.to_excel(\"results_sci_format.xlsx\", sheet_name=\"Results\")\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import Font, Alignment, Border, Side\n",
    "# 打开保存的 Excel 文件，设置列宽和基本格式\n",
    "with pd.ExcelWriter(\"results_sci_format.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    results_df_sorted.to_excel(writer, sheet_name=\"Results\")\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Results\"]\n",
    "\n",
    "    # 设置字体为 Times New Roman 并调整列宽\n",
    "    for col in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column = col[0].column_letter  # 获取列字母\n",
    "        for cell in col:\n",
    "            cell.font = Font(name=\"Times New Roman\", size=11)  # 设置字体和大小\n",
    "            if cell.value:\n",
    "                max_length = max(max_length, len(str(cell.value)))\n",
    "        # 根据内容调整列宽\n",
    "        worksheet.column_dimensions[column].width = max(max_length + 2, 15)\n",
    "\n",
    "    # 设置表头加粗\n",
    "    for cell in worksheet[1]:\n",
    "        cell.font = Font(name=\"Times New Roman\", bold=True, size=11)"
   ],
   "id": "236fa5b374be37a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
