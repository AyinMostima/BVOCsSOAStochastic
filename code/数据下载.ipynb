{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:23:05.676612Z",
     "start_time": "2024-09-23T05:23:05.106252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import threddsclient\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import numpy as np\n",
    "import time as time_module  # 避免与变量名冲突\n",
    "import cftime\n",
    "from tqdm import tqdm"
   ],
   "id": "839e6c2a328aa7f9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:23:05.684878Z",
     "start_time": "2024-09-23T05:23:05.681061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define your station codes\n",
    "station_ids = [\n",
    "    \"FR0030R\", \"GR0004U\", \"CH0005R\", \"CH0010U\", \"DE0054R\", \"CZ0003R\",\n",
    "    \"FR0020R\", \"FI0096G\", \"BE0017U\", \"BE0008U\", \"BE0012U\", \"CH0053R\",\n",
    "    \"CV0001G\", \"FR0013R\", \"FR0035U\", \"FI0039U\", \"FR0041U\", \"ES0019U\",\n",
    "    \"DE0043G\", \"IT0004R\", \"ES0025U\", \"BE0007R\", \"CY0002R\", \"NO0002R\",\n",
    "    \"DE0007R\", \"DE0008R\", \"FR0027U\", \"AT0002R\", \"FR0018R\", \"IE0031R\",\n",
    "    \"ES0021U\", \"FR0008R\", \"DE0044R\"\n",
    "]\n"
   ],
   "id": "218c8d313b64df98",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:23:17.481058Z",
     "start_time": "2024-09-23T05:23:08.141531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用户指定的时间范围\n",
    "user_start_date = datetime.strptime('20000101000000', '%Y%m%d%H%M%S')\n",
    "user_end_date = datetime.strptime('20241001000000', '%Y%m%d%H%M%S')\n",
    "# 获取所有 OPeNDAP URL\n",
    "all_opendap_urls = threddsclient.opendap_urls('https://thredds.nilu.no/thredds/catalog/ebas/catalog.xml')\n"
   ],
   "id": "85fa3cf3f02c7649",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filtered_urls = []\n",
    "# def parse_ebas_filename(filename):\n",
    "#     # 移除查询参数和扩展名\n",
    "#     filename = filename.split('?')[0]\n",
    "#     if filename.endswith('.nc'):\n",
    "#         filename = filename[:-3]\n",
    "#     parts = filename.split('.')\n",
    "#     if len(parts) < 3:\n",
    "#         return None, None, None\n",
    "#     station_id = parts[0]\n",
    "#     start_date_str = parts[1]\n",
    "#     end_date_str = parts[2]\n",
    "#     return station_id, start_date_str, end_date_str\n",
    "# \n",
    "# # 筛选符合条件的URL\n",
    "# for url in all_opendap_urls:\n",
    "#     filename = url.split('/')[-1]\n",
    "#     station_id, start_date_str, end_date_str = parse_ebas_filename(filename)\n",
    "#     if not station_id or not start_date_str or not end_date_str:\n",
    "#         continue\n",
    "#     if station_id not in station_ids:\n",
    "#         continue\n",
    "#     try:\n",
    "#         start_date = datetime.strptime(start_date_str, '%Y%m%d%H%M%S')\n",
    "#         end_date = datetime.strptime(end_date_str, '%Y%m%d%H%M%S')\n",
    "#     except ValueError:\n",
    "#         continue\n",
    "#     # 检查时间范围是否重叠\n",
    "#     if end_date < user_start_date or start_date > user_end_date:\n",
    "#         continue\n",
    "#     filtered_urls.append(url)"
   ],
   "id": "cf905912ff75ed60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:24:39.376133Z",
     "start_time": "2024-09-23T05:24:39.176517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_urls = []\n",
    "def parse_ebas_filename(filename):\n",
    "    # 移除查询参数和扩展名\n",
    "    filename = filename.split('?')[0]\n",
    "    if filename.endswith('.nc'):\n",
    "        filename = filename[:-3]\n",
    "    parts = filename.split('.')\n",
    "    if len(parts) < 3:\n",
    "        return None, None, None\n",
    "    station_id = parts[0]\n",
    "    start_date_str = parts[1]\n",
    "    end_date_str = parts[2]\n",
    "    return station_id, start_date_str, end_date_str\n",
    "\n",
    "# 筛选符合条件的URL\n",
    "for url in all_opendap_urls:\n",
    "    filename = url.split('/')[-1]\n",
    "    station_id, start_date_str, end_date_str = parse_ebas_filename(filename)\n",
    "    if not station_id or not start_date_str or not end_date_str:\n",
    "        continue\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d%H%M%S')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d%H%M%S')\n",
    "    except ValueError:\n",
    "        continue\n",
    "    # 检查时间范围是否重叠\n",
    "    if end_date < user_start_date or start_date > user_end_date:\n",
    "        continue\n",
    "    filtered_urls.append(url)"
   ],
   "id": "205b3a4a11155ede",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:24:49.486262Z",
     "start_time": "2024-09-23T05:24:49.481524Z"
    }
   },
   "cell_type": "code",
   "source": "len(filtered_urls)",
   "id": "2ca8775df9e6fd8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13169"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "isoprene_data_list = []\n",
    "temperature_data_list = []\n",
    "failed_urls = []"
   ],
   "id": "a0518c59f9ccf40a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def open_dataset_with_retry(url, retries=5, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            ds = netCDF4.Dataset(url)\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"访问数据集 {url} 失败，重试 {attempt + 1}/{retries} 次... 错误信息: {e}\")\n",
    "                time_module.sleep(delay)\n",
    "            else:\n",
    "                print(f\"无法访问数据集 {url}，已跳过。错误信息: {e}\")\n",
    "                return None\n",
    "\n",
    "# 遍历筛选后的URL，提取数据\n",
    "for url in tqdm(filtered_urls, desc=\"Processing datasets\"):\n",
    "    try:\n",
    "        ds = open_dataset_with_retry(url)\n",
    "        if ds is None:\n",
    "            failed_urls.append(url)  # 记录失败的URL\n",
    "            continue\n",
    "        variables = ds.variables\n",
    "        isoprene_vars = [\n",
    "            'isoprene_ng_per_m3_amean',\n",
    "            'isoprene_ng_per_m3_amean_qc',\n",
    "            'isoprene_ng_per_m3_precision'\n",
    "        ]\n",
    "        temperature_vars = ['temperature', 'temperature_qc']\n",
    "        has_isoprene = all(var in variables for var in isoprene_vars)\n",
    "        has_temperature = all(var in variables for var in temperature_vars)\n",
    "        if not has_isoprene and not has_temperature:\n",
    "            ds.close()\n",
    "            continue\n",
    "        # 获取元数据（调整为 netCDF4 的形式）\n",
    "        matrix = getattr(ds, 'ebas_matrix', '')\n",
    "        instrument = getattr(ds, 'ebas_instrument_type', '')\n",
    "        station_name = getattr(ds, 'ebas_station_name', '')\n",
    "        station_id = getattr(ds, 'ebas_station_code', '')\n",
    "        longitude = getattr(ds, 'ebas_station_longitude', None)\n",
    "        latitude = getattr(ds, 'ebas_station_latitude', None)\n",
    "        time_resolution = getattr(ds, 'ebas_resolution_code', '')\n",
    "        instrument_name = getattr(ds, 'ebas_instrument_name', '')\n",
    "        monitoring_equipment = getattr(ds, 'ebas_instrument_type', '')\n",
    "        # 处理时间变量\n",
    "        time_var = ds.variables.get('time')\n",
    "        if time_var is None:\n",
    "            print(f\"数据集 {url} 中不存在 'time' 变量，已跳过。\")\n",
    "            ds.close()\n",
    "            failed_urls.append(url)\n",
    "            continue\n",
    "        try:\n",
    "            time_units = getattr(time_var, 'units', '').strip()\n",
    "            time_calendar = getattr(time_var, 'calendar', 'standard')\n",
    "            if time_units:\n",
    "                # 使用 netCDF4.num2date 解析时间\n",
    "                time_values = netCDF4.num2date(\n",
    "                    time_var[:],\n",
    "                    units=time_units,\n",
    "                    calendar=time_calendar,\n",
    "                    only_use_cftime_datetimes=False,\n",
    "                    only_use_python_datetimes=True\n",
    "                )\n",
    "                # 转换为 pandas datetime\n",
    "                if isinstance(time_values[0], cftime.datetime):\n",
    "                    time_values = pd.to_datetime([t.isoformat() for t in time_values])\n",
    "                else:\n",
    "                    time_values = pd.to_datetime(time_values)\n",
    "            elif isinstance(time_var[0], (np.datetime64, datetime)):\n",
    "                # 时间变量已经是 datetime64 类型，直接转换\n",
    "                time_values = pd.to_datetime(time_var[:])\n",
    "            elif hasattr(ds, 'time_coverage_start'):\n",
    "                # 使用基准时间计算实际时间\n",
    "                base_time = pd.to_datetime(getattr(ds, 'time_coverage_start'))\n",
    "                time_values = [base_time + timedelta(seconds=float(t)) for t in time_var[:]]\n",
    "                time_values = pd.to_datetime(time_values)\n",
    "            else:\n",
    "                # 无法解析时间\n",
    "                print(f\"数据集 {url} 的时间变量无法解析，已跳过。错误信息: 无效的时间单位，且无法获取基准时间。\")\n",
    "                ds.close()\n",
    "                failed_urls.append(url)  # 记录失败的URL\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"数据集 {url} 的时间变量无法解析，已跳过。错误信息: {e}\")\n",
    "            ds.close()\n",
    "            failed_urls.append(url)  # 记录失败的URL\n",
    "            continue\n",
    "        # 确保时间是一维的\n",
    "        if np.array(time_values).ndim > 1:\n",
    "            time_values = np.array(time_values).flatten()\n",
    "        # 处理异戊二烯数据\n",
    "        if has_isoprene:\n",
    "            if matrix != 'air':\n",
    "                ds.close()\n",
    "                continue\n",
    "            if instrument not in ['online_gc', 'steel_canister', 'ads_tube', 'PTR-MS']:\n",
    "                ds.close()\n",
    "                continue\n",
    "            # 提取变量并确保是一维的\n",
    "            isoprene = ds.variables['isoprene_ng_per_m3_amean'][:]\n",
    "            isoprene_qc = ds.variables['isoprene_ng_per_m3_amean_qc'][:]\n",
    "            isoprene_precision = ds.variables['isoprene_ng_per_m3_precision'][:]\n",
    "            # 检查维度并展开为一维\n",
    "            if isoprene.ndim > 1:\n",
    "                isoprene = isoprene.flatten()\n",
    "            if isoprene_qc.ndim > 1:\n",
    "                isoprene_qc = isoprene_qc.flatten()\n",
    "            if isoprene_precision.ndim > 1:\n",
    "                isoprene_precision = isoprene_precision.flatten()\n",
    "            # 确保长度一致\n",
    "            min_length = min(len(time_values), len(isoprene), len(isoprene_qc), len(isoprene_precision))\n",
    "            time_values_iso = time_values[:min_length]\n",
    "            isoprene = isoprene[:min_length]\n",
    "            isoprene_qc = isoprene_qc[:min_length]\n",
    "            isoprene_precision = isoprene_precision[:min_length]\n",
    "            df_iso = pd.DataFrame({\n",
    "                'Date': time_values_iso,\n",
    "                'isoprene_ng_per_m3_amean': isoprene,\n",
    "                'isoprene_ng_per_m3_amean_qc': isoprene_qc,\n",
    "                'isoprene_ng_per_m3_precision': isoprene_precision,\n",
    "                'longitude': longitude,\n",
    "                'latitude': latitude,\n",
    "                'monitoring_equipment': monitoring_equipment,\n",
    "                'time_resolution': time_resolution,\n",
    "                'instrument_type': instrument,\n",
    "                'instrument_name': instrument_name,\n",
    "                'station_name': station_name,\n",
    "                'station_id': station_id\n",
    "            })\n",
    "            isoprene_data_list.append(df_iso)\n",
    "        # 处理温度数据\n",
    "        if has_temperature:\n",
    "            if matrix not in ['aerosol', 'air', 'instrument', 'met', 'pm1', 'pm10', 'pm2.5']:\n",
    "                ds.close()\n",
    "                continue\n",
    "            # 提取变量并确保是一维的\n",
    "            temperature = ds.variables['temperature'][:]\n",
    "            temperature_qc = ds.variables['temperature_qc'][:]\n",
    "            # 获取温度单位\n",
    "            temp_units = getattr(ds.variables['temperature'], 'units', '').lower()\n",
    "            # 检查维度并调整形状\n",
    "            if temperature.ndim > 1:\n",
    "                temperature = temperature.flatten()\n",
    "            if temperature_qc.ndim > 1:\n",
    "                temperature_qc = temperature_qc.flatten()\n",
    "            # 如果温度是华氏度，转换为摄氏度\n",
    "            if 'fahrenheit' in temp_units:\n",
    "                temperature = (temperature - 32) * 5.0 / 9.0\n",
    "                temp_units = 'deg c'\n",
    "            if 'k' in temp_units:\n",
    "                temperature = temperature[:] - 273.15\n",
    "                temp_units = 'deg c'\n",
    "                \n",
    "            # 确保长度一致\n",
    "            min_length = min(len(time_values), len(temperature), len(temperature_qc))\n",
    "            time_values_temp = time_values[:min_length]\n",
    "            temperature = temperature[:min_length]\n",
    "            temperature_qc = temperature_qc[:min_length]\n",
    "            df_temp = pd.DataFrame({\n",
    "                'Date': time_values_temp,\n",
    "                'temperature': temperature,\n",
    "                'temperature_qc': temperature_qc,\n",
    "                'temperature_units': temp_units,\n",
    "                'longitude': longitude,\n",
    "                'latitude': latitude,\n",
    "                'monitoring_equipment': monitoring_equipment,\n",
    "                'time_resolution': time_resolution,\n",
    "                'instrument_type': instrument,\n",
    "                'instrument_name': instrument_name,\n",
    "                'station_name': station_name,\n",
    "                'station_id': station_id\n",
    "            })\n",
    "            temperature_data_list.append(df_temp)\n",
    "        ds.close()  # 关闭数据集\n",
    "    except Exception as e:\n",
    "        print(f\"处理数据集 {url} 时出错: {e}\")\n",
    "        failed_urls.append(url)  # 记录失败的URL\n",
    "        continue\n",
    "\n",
    "# 将处理失败的URL写入txt文件\n",
    "if failed_urls:\n",
    "    with open('failed_urls.txt', 'w') as f:\n",
    "        for url in failed_urls:\n",
    "            f.write(url + '\\n')\n",
    "    print(f\"共有 {len(failed_urls)} 个数据集处理失败，已写入 failed_urls.txt。\")\n",
    "else:\n",
    "    print(\"所有数据集均处理成功。\")"
   ],
   "id": "dcb58f138e4d8c93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 合并并保存异戊二烯数据\n",
    "if isoprene_data_list:\n",
    "    isoprene_df = pd.concat(isoprene_data_list, ignore_index=True)\n",
    "    # 将 'Date' 转换为 datetime\n",
    "    isoprene_df['Date'] = pd.to_datetime(isoprene_df['Date'])\n",
    "    # 筛选时间范围内的数据\n",
    "    isoprene_df = isoprene_df[(isoprene_df['Date'] >= user_start_date) & (isoprene_df['Date'] <= user_end_date)]\n",
    "    # 保存为CSV文件\n",
    "    isoprene_df.to_csv('isoprene_data.csv', index=False)\n",
    "    # 保存为pkl文件\n",
    "    isoprene_df.to_pickle('isoprene_data.pkl')"
   ],
   "id": "d4e71e48c8687192"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 合并并保存温度数据\n",
    "if temperature_data_list:\n",
    "    temperature_df = pd.concat(temperature_data_list, ignore_index=True)\n",
    "    # 将 'Date' 转换为 datetime\n",
    "    temperature_df['Date'] = pd.to_datetime(temperature_df['Date'])\n",
    "    # 筛选时间范围内的数据\n",
    "    temperature_df = temperature_df[\n",
    "        (temperature_df['Date'] >= user_start_date) & (temperature_df['Date'] <= user_end_date)]\n",
    "    # 保存为CSV文件\n",
    "    temperature_df.to_csv('temperature_data.csv', index=False)\n",
    "    temperature_df.to_pickle('temperature_df.pkl')"
   ],
   "id": "ce9b667dd05b36a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
